{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAZq3vFDcFiT"
      },
      "source": [
        "# Dreambooth fine-tuning for Stable Diffusion using dðŸ§¨ffusers\n",
        "\n",
        "This notebook shows how to \"teach\" Stable Diffusion a new concept via Dreambooth using ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers).\n",
        "\n",
        "![Dreambooth Example](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)\n",
        "_By using just 3-5 images you can teach new concepts to Stable Diffusion and personalize the model on your own images_\n",
        "\n",
        "Differently from Textual Inversion, this approach trains the whole model, which can yield better results to the cost of bigger models.\n",
        "\n",
        "For a general introduction to the Stable Diffusion model please refer to this [colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbzZ9xe6dWwf"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "30lu8LWXmg5j",
        "cellView": "form",
        "outputId": "1ed6ee11-89d5-4fd3-9085-5a7caeee91b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title Install the required libs\n",
        "!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -qq accelerate tensorboard transformers ftfy gradio\n",
        "!pip install -qq \"ipywidgets>=7,<8\"\n",
        "!pip install -qq bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [Optional] Install xformers for faster and memory efficient training\n",
        "#@markdown Acknowledgement: The xformers wheel are taken from [TheLastBen/fast-stable-diffusion](https://github.com/TheLastBen/fast-stable-diffusion). Thanks a lot for building these wheels!\n",
        "%%time\n",
        "\n",
        "!pip install -U --pre triton\n",
        "\n",
        "from subprocess import getoutput\n",
        "from IPython.display import HTML\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'T4' in s:\n",
        "  gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "  gpu = 'P100'\n",
        "elif 'V100' in s:\n",
        "  gpu = 'V100'\n",
        "elif 'A100' in s:\n",
        "  gpu = 'A100'\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
        "        break\n",
        "    except:\n",
        "        pass\n",
        "    print('[1;31mit seems that your GPU is not supported at the moment')\n",
        "    time.sleep(5)\n",
        "\n",
        "if (gpu=='T4'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='P100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='V100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='A100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ll119uOhEUPL",
        "outputId": "07687f21-2cdb-4b00-a39d-29f9ebad27c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting triton\n",
            "  Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton) (75.2.0)\n",
            "Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "Installing collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed triton-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "f2ae245c12ae42f4ab4d99670bdaaa56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m  ERROR: HTTP error 404 while getting https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not install requirement xformers==0.0.13.dev0 from https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl because of HTTP error 404 Client Error: Not Found for url: https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl for URL https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\u001b[0m\u001b[31m\n",
            "\u001b[0mCPU times: user 5.22 s, sys: 860 ms, total: 6.08 s\n",
            "Wall time: 23.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "1_h0kO-VnQog",
        "outputId": "2c0c60e4-291c-4798-bc01-987b61acaa21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import diffusers.models.autoencoders.autoencoder_kl because of the following error (look up to see its traceback):\ncannot import name 'AttrsDescriptor' from 'triton.compiler.compiler' (/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/hints.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttrsDescriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AttrsDescriptor' from 'triton.backends.compiler' (/usr/local/lib/python3.11/dist-packages/triton/backends/compiler.py)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mautoencoder_asym_kl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsymmetricAutoencoderKL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mautoencoder_dc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoencoderDC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mautoencoder_kl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/autoencoders/autoencoder_asym_kl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoencoderKLOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiagonalGaussianDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskConditionDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusersAutoQuantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiffusersQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_transformers_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusersAutoQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusersQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbitsandbytes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBnB4BitDiffusersQuantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBnB8BitDiffusersQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgguf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGGUFQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/bitsandbytes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbnb_quantizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBnB4BitDiffusersQuantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBnB8BitDiffusersQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdequantize_and_replace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdequantize_bnb_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_with_bnb_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/bitsandbytes/bnb_quantizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_module_from_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusersQuantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantization_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantizationConfigMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/quantizers/quantization_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torchao_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_primitives\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMappingType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from torchao.quantization import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mautoquant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .autoquant import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mALL_AUTOQUANT_CLASS_LIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/quantization/autoquant.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchao\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torchao.dtypes import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mAffineQuantizedTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/dtypes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maffine_quantized_tensor_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .affine_quantized_tensor import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mAffineQuantizedTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/dtypes/affine_quantized_tensor_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torchao.dtypes.affine_quantized_tensor import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mAffineQuantizedTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/dtypes/affine_quantized_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0;31m from torchao.quantization.quant_primitives import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mFP8_TYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/quantization/quant_primitives.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m \u001b[0mregister_custom_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_custom_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_lib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchao/utils.py\u001b[0m in \u001b[0;36m_register_custom_op\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \"\"\"\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/decomposition.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dtype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mout_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_listlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from torch._prims_common import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceProperties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_inductor/runtime/hints.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttrsDescriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AttrsDescriptor' from 'triton.compiler.compiler' (/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-90155172>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDDPMScheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPNDMScheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDiffusionPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNet2DConditionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstable_diffusion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionSafetyChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import diffusers.models.autoencoders.autoencoder_kl because of the following error (look up to see its traceback):\ncannot import name 'AttrsDescriptor' from 'triton.compiler.compiler' (/usr/local/lib/python3.11/dist-packages/triton/compiler/compiler.py)"
          ]
        }
      ],
      "source": [
        "#@title Import required libraries\n",
        "import argparse\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "from contextlib import nullcontext\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import PIL\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl3r7A_3ASxm"
      },
      "source": [
        "## Settings for teaching your new concept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If5Jswe526QP"
      },
      "outputs": [],
      "source": [
        "#@markdown `pretrained_model_name_or_path` which Stable Diffusion checkpoint you want to use\n",
        "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2\" #@param [\"stabilityai/stable-diffusion-2\", \"stabilityai/stable-diffusion-2-base\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USqTimsBA96M"
      },
      "outputs": [],
      "source": [
        "#@markdown Add here the URLs to the images of the concept you are adding. 3-5 should be fine\n",
        "urls = [\n",
        "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/2.jpeg\",\n",
        "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/3.jpeg\",\n",
        "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/5.jpeg\",\n",
        "      \"https://huggingface.co/datasets/valhalla/images/resolve/main/6.jpeg\",\n",
        "      ## You can add additional images here\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "60jVYSk0BGC8"
      },
      "outputs": [],
      "source": [
        "#@title Setup and check the images you have just added\n",
        "import requests\n",
        "import glob\n",
        "from io import BytesIO\n",
        "\n",
        "def download_image(url):\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "  except:\n",
        "    return None\n",
        "  return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "images = list(filter(None,[download_image(url) for url in urls]))\n",
        "save_path = \"./my_concept\"\n",
        "if not os.path.exists(save_path):\n",
        "  os.mkdir(save_path)\n",
        "[image.save(f\"{save_path}/{i}.jpeg\") for i, image in enumerate(images)]\n",
        "image_grid(images, 1, len(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8i_vLTBxAXpE"
      },
      "outputs": [],
      "source": [
        "#@title Settings for your newly created concept\n",
        "#@markdown `instance_prompt` is a prompt that should contain a good description of what your object or style is, together with the initializer word `cat_toy`\n",
        "instance_prompt = \"<cat-toy> toy\" #@param {type:\"string\"}\n",
        "#@markdown Check the `prior_preservation` option if you would like class of the concept (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\n",
        "prior_preservation = False #@param {type:\"boolean\"}\n",
        "prior_preservation_class_prompt = \"a photo of a cat clay toy\" #@param {type:\"string\"}\n",
        "\n",
        "num_class_images = 12\n",
        "sample_batch_size = 2\n",
        "prior_loss_weight = 0.5\n",
        "prior_preservation_class_folder = \"./class_images\"\n",
        "class_data_root=prior_preservation_class_folder\n",
        "class_prompt=prior_preservation_class_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCuPKFeMLe9S"
      },
      "source": [
        "#### Advanced settings for prior preservation (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8SQ259CK3Cd"
      },
      "outputs": [],
      "source": [
        "num_class_images = 12 #@param {type: \"number\"}\n",
        "sample_batch_size = 2\n",
        "#@markdown `prior_preservation_weight` determins how strong the class for prior preservation should be\n",
        "prior_loss_weight = 1 #@param {type: \"number\"}\n",
        "\n",
        "\n",
        "#@markdown If the `prior_preservation_class_folder` is empty, images for the class will be generated with the class prompt. Otherwise, fill this folder with images of items on the same class as your concept (but not images of the concept itself)\n",
        "prior_preservation_class_folder = \"./class_images\" #@param {type:\"string\"}\n",
        "class_data_root=prior_preservation_class_folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D633UIuGgs6M"
      },
      "source": [
        "## Teach the model the new concept (fine-tuning with Dreambooth)\n",
        "Execute this this sequence of cells to run the training process. The whole process may take from 15 min to 2 hours. (Open this block if you are interested in how this process works under the hood or if you want to change advanced training settings or hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lzxceJuASiev"
      },
      "outputs": [],
      "source": [
        "#@title Setup the Classes\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        instance_data_root,\n",
        "        instance_prompt,\n",
        "        tokenizer,\n",
        "        class_data_root=None,\n",
        "        class_prompt=None,\n",
        "        size=512,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.instance_data_root = Path(instance_data_root)\n",
        "        if not self.instance_data_root.exists():\n",
        "            raise ValueError(\"Instance images root doesn't exists.\")\n",
        "\n",
        "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.instance_prompt = instance_prompt\n",
        "        self._length = self.num_instance_images\n",
        "\n",
        "        if class_data_root is not None:\n",
        "            self.class_data_root = Path(class_data_root)\n",
        "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
        "            self.class_images_path = list(Path(class_data_root).iterdir())\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_class_images, self.num_instance_images)\n",
        "            self.class_prompt = class_prompt\n",
        "        else:\n",
        "            self.class_data_root = None\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_data_root:\n",
        "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                self.class_prompt,\n",
        "                padding=\"do_not_pad\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "\n",
        "        return example\n",
        "\n",
        "\n",
        "class PromptDataset(Dataset):\n",
        "    def __init__(self, prompt, num_samples):\n",
        "        self.prompt = prompt\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        example[\"prompt\"] = self.prompt\n",
        "        example[\"index\"] = index\n",
        "        return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mneZ4Ct2BenE"
      },
      "outputs": [],
      "source": [
        "#@title Generate Class Images\n",
        "import gc\n",
        "if(prior_preservation):\n",
        "    class_images_dir = Path(class_data_root)\n",
        "    if not class_images_dir.exists():\n",
        "        class_images_dir.mkdir(parents=True)\n",
        "    cur_class_images = len(list(class_images_dir.iterdir()))\n",
        "\n",
        "    if cur_class_images < num_class_images:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            pretrained_model_name_or_path, revision=\"fp16\", torch_dtype=torch.float16\n",
        "        ).to(\"cuda\")\n",
        "        pipeline.enable_attention_slicing()\n",
        "        pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "        num_new_images = num_class_images - cur_class_images\n",
        "        print(f\"Number of class images to sample: {num_new_images}.\")\n",
        "\n",
        "        sample_dataset = PromptDataset(class_prompt, num_new_images)\n",
        "        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=sample_batch_size)\n",
        "\n",
        "        for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n",
        "            images = pipeline(example[\"prompt\"]).images\n",
        "\n",
        "            for i, image in enumerate(images):\n",
        "                image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n",
        "        pipeline = None\n",
        "        gc.collect()\n",
        "        del pipeline\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gIFaJum5nqeo"
      },
      "outputs": [],
      "source": [
        "#@title Load the Stable Diffusion model\n",
        "# Load models and create wrapper for stable diffusion\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
        ")\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
        ")\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
        ")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    subfolder=\"tokenizer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4ayDcwEHDa4"
      },
      "outputs": [],
      "source": [
        "#@title Setting up all training args\n",
        "from argparse import Namespace\n",
        "args = Namespace(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    resolution=vae.sample_size,\n",
        "    center_crop=True,\n",
        "    train_text_encoder=False,\n",
        "    instance_data_dir=save_path,\n",
        "    instance_prompt=instance_prompt,\n",
        "    learning_rate=5e-06,\n",
        "    max_train_steps=300,\n",
        "    save_steps=50,\n",
        "    train_batch_size=2, # set to 1 if using prior preservation\n",
        "    gradient_accumulation_steps=2,\n",
        "    max_grad_norm=1.0,\n",
        "    mixed_precision=\"fp16\", # set to \"fp16\" for mixed-precision training.\n",
        "    gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "    use_8bit_adam=True, # use 8bit optimizer from bitsandbytes\n",
        "    seed=3434554,\n",
        "    with_prior_preservation=prior_preservation,\n",
        "    prior_loss_weight=prior_loss_weight,\n",
        "    sample_batch_size=2,\n",
        "    class_data_dir=prior_preservation_class_folder,\n",
        "    class_prompt=prior_preservation_class_prompt,\n",
        "    num_class_images=num_class_images,\n",
        "    lr_scheduler=\"constant\",\n",
        "    lr_warmup_steps=100,\n",
        "    output_dir=\"dreambooth-concept\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lKGmcIyJbCu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training function\n",
        "from accelerate.utils import set_seed\n",
        "def training_function(text_encoder, vae, unet):\n",
        "    logger = get_logger(__name__)\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "    )\n",
        "\n",
        "    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n",
        "    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n",
        "    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n",
        "    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n",
        "        raise ValueError(\n",
        "            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n",
        "            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n",
        "        )\n",
        "\n",
        "    vae.requires_grad_(False)\n",
        "    if not args.train_text_encoder:\n",
        "        text_encoder.requires_grad_(False)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "        if args.train_text_encoder:\n",
        "            text_encoder.gradient_checkpointing_enable()\n",
        "\n",
        "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
        "    if args.use_8bit_adam:\n",
        "        optimizer_class = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_class = torch.optim.AdamW\n",
        "\n",
        "    params_to_optimize = (\n",
        "        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\n",
        "    )\n",
        "\n",
        "    optimizer = optimizer_class(\n",
        "        params_to_optimize,\n",
        "        lr=args.learning_rate,\n",
        "    )\n",
        "\n",
        "    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "    train_dataset = DreamBoothDataset(\n",
        "        instance_data_root=args.instance_data_dir,\n",
        "        instance_prompt=args.instance_prompt,\n",
        "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
        "        class_prompt=args.class_prompt,\n",
        "        tokenizer=tokenizer,\n",
        "        size=args.resolution,\n",
        "        center_crop=args.center_crop,\n",
        "    )\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "\n",
        "        # concat class and instance examples for prior preservation\n",
        "        if args.with_prior_preservation:\n",
        "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "            pixel_values += [example[\"class_images\"] for example in examples]\n",
        "\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "\n",
        "        input_ids = tokenizer.pad(\n",
        "            {\"input_ids\": input_ids},\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length\n",
        "        ).input_ids\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"pixel_values\": pixel_values,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        args.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
        "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    if args.train_text_encoder:\n",
        "        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "    else:\n",
        "        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            unet, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    # Move text_encode and vae to gpu.\n",
        "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
        "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "    vae.decoder.to(\"cpu\")\n",
        "    if not args.train_text_encoder:\n",
        "        text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(\"Steps\")\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_train_epochs):\n",
        "        unet.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Convert images to latent space\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
        "                latents = latents * 0.18215\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                # Predict the noise residual\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                # Get the target for loss depending on the prediction type\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                if args.with_prior_preservation:\n",
        "                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n",
        "                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "                    target, target_prior = torch.chunk(target, 2, dim=0)\n",
        "\n",
        "                    # Compute instance loss\n",
        "                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                    # Compute prior loss\n",
        "                    prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
        "\n",
        "                    # Add the prior loss to the instance loss.\n",
        "                    loss = loss + args.prior_loss_weight * prior_loss\n",
        "                else:\n",
        "                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    params_to_clip = (\n",
        "                        itertools.chain(unet.parameters(), text_encoder.parameters())\n",
        "                        if args.train_text_encoder\n",
        "                        else unet.parameters()\n",
        "                    )\n",
        "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "                if global_step % args.save_steps == 0:\n",
        "                    if accelerator.is_main_process:\n",
        "                        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                            args.pretrained_model_name_or_path,\n",
        "                            unet=accelerator.unwrap_model(unet),\n",
        "                            text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "                        )\n",
        "                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
        "                        pipeline.save_pretrained(save_path)\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item()}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "        accelerator.wait_for_everyone()\n",
        "\n",
        "    # Create the pipeline using using the trained modules and save it.\n",
        "    if accelerator.is_main_process:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            args.pretrained_model_name_or_path,\n",
        "            unet=accelerator.unwrap_model(unet),\n",
        "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mfeKJn_LJi_V"
      },
      "outputs": [],
      "source": [
        "#@title Run training\n",
        "import accelerate\n",
        "accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n",
        "for param in itertools.chain(unet.parameters(), text_encoder.parameters()):\n",
        "  if param.grad is not None:\n",
        "    del param.grad  # free some memory\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50JuJUM8EG1h"
      },
      "source": [
        "## Run the code with your newly trained model\n",
        "If you have just trained your model with the code above, use the block below to run it.\n",
        "\n",
        "Also explore the [DreamBooth Concepts Library](https://huggingface.co/sd-dreambooth-library)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gTlUJYB1QNSN"
      },
      "outputs": [],
      "source": [
        "#@title Save your newly created concept? you may save it privately to your personal profile or collaborate to the [library of concepts](https://huggingface.co/sd-dreambooth-library)?\n",
        "#@markdown If you wish your model to be avaliable for everyone, add it to the public library. If you prefer to use your model privately, add your own profile.\n",
        "\n",
        "save_concept = True #@param {type:\"boolean\"}\n",
        "#@markdown Once you save it you can use your concept by loading the model on any `from_pretrained` function\n",
        "name_of_your_concept = \"Cat toy\" #@param {type:\"string\"}\n",
        "where_to_save_concept = \"public_library\" #@param [\"public_library\", \"privately_to_my_profile\"]\n",
        "\n",
        "#@markdown `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)\n",
        "hf_token_write = \"\" #@param {type:\"string\"}\n",
        "if(save_concept):\n",
        "  from slugify import slugify\n",
        "  from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "  from huggingface_hub import create_repo\n",
        "  from IPython.display import display_markdown\n",
        "  api = HfApi()\n",
        "  your_username = api.whoami()[\"name\"]\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    args.output_dir,\n",
        "    torch_dtype=torch.float16,\n",
        "  ).to(\"cuda\")\n",
        "  os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "  pipe.save_pretrained(\"fp16_model\")\n",
        "\n",
        "  if(where_to_save_concept == \"public_library\"):\n",
        "    repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n",
        "    #Join the Concepts Library organization if you aren't part of it already\n",
        "    !curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\n",
        "  else:\n",
        "    repo_id = f\"{your_username}/{slugify(name_of_your_concept)}\"\n",
        "  output_dir = args.output_dir\n",
        "  if(not hf_token_write):\n",
        "    with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "  else:\n",
        "    hf_token = hf_token_write\n",
        "\n",
        "  images_upload = os.listdir(\"my_concept\")\n",
        "  image_string = \"\"\n",
        "  #repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n",
        "  for i, image in enumerate(images_upload):\n",
        "      image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
        "'''\n",
        "  readme_text = f'''---\n",
        "license: creativeml-openrail-m\n",
        "tags:\n",
        "- text-to-image\n",
        "---\n",
        "### {name_of_your_concept} on Stable Diffusion via Dreambooth\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.\n",
        "It can be used by modifying the `instance_prompt`: **{instance_prompt}**\n",
        "\n",
        "You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n",
        "\n",
        "Here are the images used for training this concept:\n",
        "{image_string}\n",
        "'''\n",
        "  #Save the readme to a file\n",
        "  readme_file = open(\"README.md\", \"w\")\n",
        "  readme_file.write(readme_text)\n",
        "  readme_file.close()\n",
        "  #Save the token identifier to a file\n",
        "  text_file = open(\"token_identifier.txt\", \"w\")\n",
        "  text_file.write(instance_prompt)\n",
        "  text_file.close()\n",
        "  operations = [\n",
        "    CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "    CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "  ]\n",
        "  create_repo(repo_id,private=True, token=hf_token)\n",
        "\n",
        "  api.create_commit(\n",
        "    repo_id=repo_id,\n",
        "    operations=operations,\n",
        "    commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",\n",
        "    token=hf_token\n",
        "  )\n",
        "  api.upload_folder(\n",
        "    folder_path=\"fp16_model\",\n",
        "    path_in_repo=\"\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        "  )\n",
        "  api.upload_folder(\n",
        "    folder_path=save_path,\n",
        "    path_in_repo=\"concept_images\",\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token\n",
        "  )\n",
        "display_markdown(f'''## Your concept was saved successfully. [Click here to access it](https://huggingface.co/{repo_id})\n",
        "''', raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2CMlPbOeEC09"
      },
      "outputs": [],
      "source": [
        "#@title Set up the pipeline\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "try:\n",
        "    pipe\n",
        "except NameError:\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        args.output_dir,\n",
        "        scheduler = DPMSolverMultistepScheduler.from_pretrained(args.output_dir, subfolder=\"scheduler\"),\n",
        "        torch_dtype=torch.float16,\n",
        "    ).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rscg285SBh4M"
      },
      "outputs": [],
      "source": [
        "#@title Run the Stable Diffusion pipeline with interactive UI Demo on Gradio\n",
        "#@markdown Run this cell to get an interactive demo where you can run the model using Gradio\n",
        "\n",
        "#@markdown ![](https://i.imgur.com/2ACLWu2.png)\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, num_samples):\n",
        "    all_images = []\n",
        "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=25).images\n",
        "    all_images.extend(images)\n",
        "    return all_images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"prompt\")\n",
        "            samples = gr.Slider(label=\"Samples\",value=1)\n",
        "            run = gr.Button(value=\"Run\")\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery(show_label=False)\n",
        "\n",
        "    run.click(inference, inputs=[prompt,samples], outputs=gallery)\n",
        "    gr.Examples([[\"a photo of sks toy riding a bicycle\", 1,1]], [prompt,samples], gallery, inference, cache_examples=False)\n",
        "\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3UREGd7EkLh"
      },
      "outputs": [],
      "source": [
        "#@title Run the Stable Diffusion pipeline on Colab\n",
        "#@markdown Don't forget to use the placeholder token in your prompt\n",
        "\n",
        "prompt = \"a \\u003Ccat-toy> in mad max fury road\" #@param {type:\"string\"}\n",
        "\n",
        "num_samples = 2 #@param {type:\"number\"}\n",
        "num_rows = 1 #@param {type:\"number\"}\n",
        "\n",
        "all_images = []\n",
        "for _ in range(num_rows):\n",
        "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=25, guidance_scale=9).images\n",
        "    all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, num_rows, num_samples)\n",
        "grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXTRkl2wB9w-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NCuPKFeMLe9S",
        "D633UIuGgs6M"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('3.7.9')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "76721e0cd9246c299eb22246d1f3c601ec1aef6bd84d45d2547549094e7b6fb7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}